# ArXiv Summary
## 1. Introduction
### 
## 2. Preliminaries
### 
### 2.1. Pre-tokenization Methods
#### - Pre-tokenization is a process of breaking text into chunks and then tokenizing them.
### 2.2. Vocabulary Construction
#### Summary: The dialogue discusses Byte-Level, Lossless Subword Tokenization methods. There is discussion on BPE, WordPiece, and Unigram as baseline subword tokenizers, as well as SaGe.
### 2.3. Segmentation Methods
#### - Given a tokenizer and a vocabulary of tokens, segmentation converts text into a series of tokens. 
## 3. PathPiece
### - PathPiece is a lossless subword tokenizer that competes with BPE and GPT-3.
### 3.1. Segmentation
#### Summary: PathPiece is a tool for tokenization. PathPiece works on a directed acyclic graph representing training data. 
### 3.2. Vocabulary Construction
#### 
### 3.3. ConnectingPathPieceand Unigram
#### 
## 4. Experiments
### 
### 4.1. Downstream Evaluation Tasks
#### 
### 4.2. Tokenization Stage Variants
#### 
## 5. Results
### Table 10 from Appendix G shows the downstream performance across all their experimental settings.
### Summary:
Summary: Table 10 from Appendix G shows the downstream performance across all their experimental settings.
### 5.1. Vocabulary Size
#### The overall average accuracy for each variant is 0.750, 0.801 and 0.834. For this reason the vocabulary size is not crucial decision.
### 5.2. Overall performance
#### - There is no tokenizer better than the other one at the 350M model size.
### 5.3. Corpus Token Count vs Accuracy
#### 
## 6. Analysis
### 
### 6.1. Pre-tokenization
#### 
### 6.2. Vocabulary Construction
#### 
### 6.3. Initial Vocabulary
#### - PathPiece and SaGe both need an initial vocabulary.
### 6.4. Effect of Model Size
#### - To examine the dependency on model size, they build larger models of 1.3B parameters for 6 of them and 2.4B parameters for 4 of them. These models were trained over the 200 billion tokens and tested at a vocabulary size of 40,960. 
## 7. Conclusion
### 
